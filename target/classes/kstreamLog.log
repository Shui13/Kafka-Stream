[main] INFO  com.aps.akamill.StreamProcessor  - Started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bufferpool-wait-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name buffer-exhausted-records
[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-size
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name compression-rate
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name queue-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name request-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name produce-throttle-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-per-request
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-retries
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name errors
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-size-max
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Starting Kafka producer I/O thread.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - Kafka producer started
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Starting the Kafka consumer
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Kafka consumer created
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Starting the Kafka consumer
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Kafka consumer created
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name poll-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name process-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name punctuate-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name task-creation
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name task-destruction
[main] DEBUG org.apache.kafka.streams.KafkaStreams  - Starting Kafka Stream process
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Subscribed to topic(s): gin
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending GroupCoordinator request for group sadness to broker dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -2 at dev-anta-tools03.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -2.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -2.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initialize connection to node -1 for sending metadata request
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -1 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -1
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -1.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -1.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initialize connection to node -3 for sending metadata request
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -3 at dev-anta-tools04.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -2: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Sending metadata request (type=MetadataRequest, topics=gin) to node -2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -3
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -3.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -3.
[StreamThread-1] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 2 to Cluster(id = PQR0Qky-QBWw7OpyQnrV1g, nodes = [dev-anta-tools03.kendall.corp.akamai.com:9092 (id: 3 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 4 rack: null)], partitions = [Partition(topic = gin, partition = 0, leader = 2, replicas = [2,3,4], isr = [2,3,4])])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received GroupCoordinator response ClientResponse(receivedTimeMs=1495637108039, latencyMs=739, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=sadness-1-StreamThread-1-consumer}, responseBody={error_code=0,coordinator={node_id=2,host=dev-anta-tools02.kendall.corp.akamai.com,port=9092}}) for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node 2147483645 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Heartbeat thread for group sadness started
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending JoinGroup ((type: JoinGroupRequest, groupId=sadness, sessionTimeout=10000, rebalanceTimeout=300000, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@2ff6137a)) to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -3: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node 2147483645
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node 2147483645.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node 2147483645.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node 2147483645: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful JoinGroup response for group sadness: {error_code=0,generation_id=11,group_protocol=stream,leader_id=sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731,member_id=sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731,members=[{member_id=sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=43 cap=43]}]}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Performing assignment for group sadness using strategy stream with subscriptions {sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731=Subscription(topics=[gin])}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Finished assignment for group sadness: {sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731=Assignment(partitions=[gin-0])}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending leader SyncGroup for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null): (type=SyncGroupRequest, groupId=sadness, generationId=11, memberId=sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731, groupAssignment=sadness-1-StreamThread-1-consumer-6f3edaf5-ad05-42b5-95ed-467130524731)
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 11
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Group sadness fetching committed offsets for partitions: [gin-0]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Group sadness has no committed offset for partition gin-0
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Resetting offset for partition gin-0 to latest offset.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node 2 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node 2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node 2.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node 2.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node 2: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Handling ListOffsetResponse response for gin-0. Fetched offset 65, timestamp -1
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.gin.bytes-fetched
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.gin.records-fetched
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name gin-0.records-lag
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[Thread-1] DEBUG org.apache.kafka.streams.KafkaStreams  - Stopping Kafka Stream process
[StreamThread-1] DEBUG org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down at user request.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Beginning shutdown of Kafka producer I/O thread, sending remaining records.
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Shutdown of Kafka producer I/O thread has completed.
[StreamThread-1] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - The Kafka producer has closed.
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending LeaveGroup request to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - LeaveGroup request for group sadness returned successfully
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Heartbeat thread for group sadness has closed
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.latency
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - The Kafka consumer has closed.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - The Kafka consumer has closed.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bufferpool-wait-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name buffer-exhausted-records
[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null), dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-size
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name compression-rate
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name queue-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name request-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name produce-throttle-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-per-request
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-retries
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name errors
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-size-max
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Starting Kafka producer I/O thread.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - Kafka producer started
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Starting the Kafka consumer
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Kafka consumer created
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Starting the Kafka consumer
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Kafka consumer created
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name poll-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name process-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name punctuate-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name task-creation
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name task-destruction
[main] DEBUG org.apache.kafka.streams.KafkaStreams  - Starting Kafka Stream process
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Subscribed to topic(s): gin
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending GroupCoordinator request for group sadness to broker dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -3 at dev-anta-tools04.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -3
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -3.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -3.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initialize connection to node -1 for sending metadata request
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -1 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -1
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -1.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -1.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initialize connection to node -2 for sending metadata request
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -2 at dev-anta-tools03.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -3: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Sending metadata request (type=MetadataRequest, topics=gin) to node -3
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -2.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -2.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 2 to Cluster(id = PQR0Qky-QBWw7OpyQnrV1g, nodes = [dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 4 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null), dev-anta-tools03.kendall.corp.akamai.com:9092 (id: 3 rack: null)], partitions = [Partition(topic = gin, partition = 0, leader = 2, replicas = [2,3,4], isr = [2,3,4])])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received GroupCoordinator response ClientResponse(receivedTimeMs=1495637142792, latencyMs=825, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=sadness-1-StreamThread-1-consumer}, responseBody={error_code=0,coordinator={node_id=2,host=dev-anta-tools02.kendall.corp.akamai.com,port=9092}}) for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node 2147483645 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Heartbeat thread for group sadness started
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending JoinGroup ((type: JoinGroupRequest, groupId=sadness, sessionTimeout=10000, rebalanceTimeout=300000, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@4d0380dc)) to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -2: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node 2147483645
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node 2147483645.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node 2147483645.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node 2147483645: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful JoinGroup response for group sadness: {error_code=0,generation_id=13,group_protocol=stream,leader_id=sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea,member_id=sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea,members=[{member_id=sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=43 cap=43]}]}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Performing assignment for group sadness using strategy stream with subscriptions {sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea=Subscription(topics=[gin])}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Finished assignment for group sadness: {sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea=Assignment(partitions=[gin-0])}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending leader SyncGroup for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null): (type=SyncGroupRequest, groupId=sadness, generationId=13, memberId=sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea, groupAssignment=sadness-1-StreamThread-1-consumer-2a23aba1-b127-42bc-9eae-8da708e20bea)
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 13
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Group sadness fetching committed offsets for partitions: [gin-0]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Group sadness has no committed offset for partition gin-0
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Resetting offset for partition gin-0 to latest offset.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node 2 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node 2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node 2.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node 2.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node 2: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Handling ListOffsetResponse response for gin-0. Fetched offset 65, timestamp -1
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.gin.bytes-fetched
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.gin.records-fetched
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name gin-0.records-lag
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[Thread-1] DEBUG org.apache.kafka.streams.KafkaStreams  - Stopping Kafka Stream process
[StreamThread-1] DEBUG org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down at user request.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Beginning shutdown of Kafka producer I/O thread, sending remaining records.
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Shutdown of Kafka producer I/O thread has completed.
[StreamThread-1] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - The Kafka producer has closed.
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending LeaveGroup request to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - LeaveGroup request for group sadness returned successfully
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Heartbeat thread for group sadness has closed
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.latency
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - The Kafka consumer has closed.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - The Kafka consumer has closed.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] ERROR com.aps.akamill.StreamProcessor  - Started
[main] DEBUG com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bufferpool-wait-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name buffer-exhausted-records
[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name batch-size
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name compression-rate
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name queue-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name request-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name produce-throttle-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-per-request
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-retries
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name errors
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name record-size-max
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Starting Kafka producer I/O thread.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - Kafka producer started
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Starting the Kafka consumer
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null), dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Kafka consumer created
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Starting the Kafka consumer
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 1 to Cluster(id = null, nodes = [dev-anta-tools03.kendall.corp.akamai.com:9092 (id: -2 rack: null), dev-anta-tools02.kendall.corp.akamai.com:9092 (id: -1 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null)], partitions = [])
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-closed:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name connections-created:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-sent:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-received:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name select-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name io-time:
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name heartbeat-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name join-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name sync-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name bytes-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-fetched
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-latency
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name records-lag
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name fetch-throttle-time
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Kafka consumer created
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name commit-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name poll-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name process-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name punctuate-time
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name task-creation
[main] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name task-destruction
[main] DEBUG org.apache.kafka.streams.KafkaStreams  - Starting Kafka Stream process
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Subscribed to topic(s): gin
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending GroupCoordinator request for group sadness to broker dev-anta-tools04.kendall.corp.akamai.com:9092 (id: -3 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -3 at dev-anta-tools04.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--3.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -3
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -3.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -3.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initialize connection to node -2 for sending metadata request
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -2 at dev-anta-tools03.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -2.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -2.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initialize connection to node -1 for sending metadata request
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node -1 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -3: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Sending metadata request (type=MetadataRequest, topics=gin) to node -3
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node--1.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node -1
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node -1.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node -1.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -2: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.Metadata  - Updated cluster metadata version 2 to Cluster(id = PQR0Qky-QBWw7OpyQnrV1g, nodes = [dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null), dev-anta-tools03.kendall.corp.akamai.com:9092 (id: 3 rack: null), dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 4 rack: null)], partitions = [Partition(topic = gin, partition = 0, leader = 2, replicas = [2,3,4], isr = [2,3,4])])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received GroupCoordinator response ClientResponse(receivedTimeMs=1495719401483, latencyMs=958, disconnected=false, requestHeader={api_key=10,api_version=0,correlation_id=0,client_id=sadness-1-StreamThread-1-consumer}, responseBody={error_code=0,coordinator={node_id=2,host=dev-anta-tools02.kendall.corp.akamai.com,port=9092}}) for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node 2147483645 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Heartbeat thread for group sadness started
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending JoinGroup ((type: JoinGroupRequest, groupId=sadness, sessionTimeout=10000, rebalanceTimeout=300000, memberId=, protocolType=consumer, groupProtocols=org.apache.kafka.common.requests.JoinGroupRequest$ProtocolMetadata@45bf9125)) to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node -1: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2147483645.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node 2147483645
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node 2147483645.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node 2147483645.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node 2147483645: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful JoinGroup response for group sadness: {error_code=0,generation_id=5,group_protocol=stream,leader_id=sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f,member_id=sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f,members=[{member_id=sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f,member_metadata=java.nio.HeapByteBuffer[pos=0 lim=43 cap=43]}]}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Performing assignment for group sadness using strategy stream with subscriptions {sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f=Subscription(topics=[gin])}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Finished assignment for group sadness: {sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f=Assignment(partitions=[gin-0])}
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending leader SyncGroup for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null): (type=SyncGroupRequest, groupId=sadness, generationId=5, memberId=sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f, groupAssignment=sadness-1-StreamThread-1-consumer-342252df-c668-4d14-bcad-d0cbc0cc5c7f)
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 5
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Group sadness fetching committed offsets for partitions: [gin-0]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Group sadness has no committed offset for partition gin-0
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Resetting offset for partition gin-0 to latest offset.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating connection to node 2 at dev-anta-tools02.kendall.corp.akamai.com:9092.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name node-2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.network.Selector  - Created socket with SO_RCVBUF = 66052, SO_SNDBUF = 132104, SO_TIMEOUT = 0 to node 2
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Completed connection to node 2.  Fetching API versions.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Initiating API versions fetch from node 2.
[StreamThread-1] DEBUG org.apache.kafka.clients.NetworkClient  - Recorded API versions for node 2: (Produce(0): 0 to 2 [usable: 2], Fetch(1): 0 to 3 [usable: 3], Offsets(2): 0 to 1 [usable: 1], Metadata(3): 0 to 2 [usable: 2], LeaderAndIsr(4): 0 [usable: 0], StopReplica(5): 0 [usable: 0], UpdateMetadata(6): 0 to 3 [usable: 3], ControlledShutdown(7): 1 [usable: 1], OffsetCommit(8): 0 to 2 [usable: 2], OffsetFetch(9): 0 to 2 [usable: 2], GroupCoordinator(10): 0 [usable: 0], JoinGroup(11): 0 to 1 [usable: 1], Heartbeat(12): 0 [usable: 0], LeaveGroup(13): 0 [usable: 0], SyncGroup(14): 0 [usable: 0], DescribeGroups(15): 0 [usable: 0], ListGroups(16): 0 [usable: 0], SaslHandshake(17): 0 [usable: 0], ApiVersions(18): 0 [usable: 0], CreateTopics(19): 0 to 1 [usable: 1], DeleteTopics(20): 0 [usable: 0])
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Handling ListOffsetResponse response for gin-0. Fetched offset 65, timestamp -1
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.gin.bytes-fetched
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name topic.gin.records-fetched
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Added sensor with name gin-0.records-lag
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending Heartbeat request for group sadness to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Received successful Heartbeat response for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.Fetcher  - Sending fetch for partitions [gin-0] to broker dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2 rack: null)
[Thread-1] DEBUG org.apache.kafka.streams.KafkaStreams  - Stopping Kafka Stream process
[StreamThread-1] DEBUG org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down at user request.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - Unsubscribed all topics or patterns and assigned partitions
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Beginning shutdown of Kafka producer I/O thread, sending remaining records.
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] DEBUG org.apache.kafka.clients.producer.internals.Sender  - Shutdown of Kafka producer I/O thread has completed.
[StreamThread-1] DEBUG org.apache.kafka.clients.producer.KafkaProducer  - The Kafka producer has closed.
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Sending LeaveGroup request to coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - LeaveGroup request for group sadness returned successfully
[kafka-coordinator-heartbeat-thread | sadness] DEBUG org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Heartbeat thread for group sadness has closed
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--3.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--2.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node--1.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2147483645.latency
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.bytes-sent
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.bytes-received
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name node-2.latency
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - The Kafka consumer has closed.
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-closed:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name connections-created:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-sent:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name bytes-received:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name select-time:
[StreamThread-1] DEBUG org.apache.kafka.common.metrics.Metrics  - Removed sensor with name io-time:
[StreamThread-1] DEBUG org.apache.kafka.clients.consumer.KafkaConsumer  - The Kafka consumer has closed.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 7
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 1
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 25
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 10
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 11
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 10
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 21
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 10
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 31
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 3
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 32
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 33
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 34
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 7
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 5
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 8
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 11
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 13
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 15
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 17
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 20
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 23
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 25
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[kafka-coordinator-heartbeat-thread | sadness] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Marking the coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) dead for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 27
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 23
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 29
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 1
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 24
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.StreamThread  - Streams application error during processing in thread [StreamThread-1]: 
java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Integer
	at com.aps.akamill.StreamProcessor.lambda$main$1(StreamProcessor.java:145)
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:42)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:64)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:174)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:320)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 3
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 35
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Cpcode Not found: 365048
[StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.StreamThread  - Streams application error during processing in thread [StreamThread-1]: 
java.lang.NullPointerException
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:64)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:174)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:320)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 1
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 30
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - false
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Cpcode Not found: 365048
[StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.StreamThread  - Streams application error during processing in thread [StreamThread-1]: 
java.lang.NullPointerException
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:64)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:174)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:320)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 3
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 34
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - false
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Cpcode Not found: 365048
[StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.StreamThread  - Streams application error during processing in thread [StreamThread-1]: 
java.lang.NullPointerException
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:64)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:174)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:320)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 5
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 30
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Cpcode Not found: 365048
[StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.StreamThread  - Streams application error during processing in thread [StreamThread-1]: 
java.lang.NullPointerException
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:64)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:174)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:320)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 1
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 35
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 3
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 23
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 5
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 23
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 7
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 22
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:pingu
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:pingu
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 1
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 3
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 23
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Cpcode Not found: 365048
[StreamThread-1] ERROR org.apache.kafka.streams.processor.internals.StreamThread  - Streams application error during processing in thread [StreamThread-1]: 
java.lang.NullPointerException
	at org.apache.kafka.streams.kstream.internals.KStreamMap$KStreamMapProcessor.process(KStreamMap.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.kstream.internals.KStreamFlatMapValues$KStreamFlatMapValuesProcessor.process(KStreamFlatMapValues.java:43)
	at org.apache.kafka.streams.processor.internals.ProcessorNode.process(ProcessorNode.java:68)
	at org.apache.kafka.streams.processor.internals.StreamTask.forward(StreamTask.java:338)
	at org.apache.kafka.streams.processor.internals.ProcessorContextImpl.forward(ProcessorContextImpl.java:187)
	at org.apache.kafka.streams.processor.internals.SourceNode.process(SourceNode.java:64)
	at org.apache.kafka.streams.processor.internals.StreamTask.process(StreamTask.java:174)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:320)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 5
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 25
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 9
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 21
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"307","record_tag":"r","timestamp":1.497545578E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"38","record_tag":"r","timestamp":1.497545958E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"60","record_tag":"r","timestamp":1.497546297E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"1f4","record_tag":"r","timestamp":1.497546468E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"164","record_tag":"r","timestamp":1.497546472E12}
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 7
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 26
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 9
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 23
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 11
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 24
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[kafka-producer-network-thread | sadness-1-StreamThread-1-producer] WARN  org.apache.kafka.clients.NetworkClient  - Error while fetching metadata with correlation id 2 : {s4=LEADER_NOT_AVAILABLE}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 5
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = sadness
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = sadness-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = sadness
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools02.kendall.corp.akamai.com:9092, dev-anta-tools03.kendall.corp.akamai.com:9092, dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = sadness-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools02.kendall.corp.akamai.com:9092 (id: 2147483645 rack: null) for group sadness.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group sadness
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group sadness with generation 13
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group sadness
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 25
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 11
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 23
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"db","record_tag":"r","timestamp":1.497547222E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"13d","record_tag":"r","timestamp":1.497547468E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"b8","record_tag":"r","timestamp":1.497547559E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"3d6","record_tag":"r","timestamp":1.497547629E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"2e6","record_tag":"r","timestamp":1.497547631E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"34c","record_tag":"r","timestamp":1.497547632E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 7
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 7
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"175","record_tag":"r","timestamp":1.497547633E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 8
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 8
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:shittwo
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"d4","record_tag":"r","timestamp":1.497547686E12}
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
[main] INFO  com.aps.akamill.StreamProcessor  - Kafka stream started
[main] INFO  org.apache.kafka.streams.StreamsConfig  - StreamsConfig values: 
	application.id = dobby
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffered.records.per.partition = 1000
	client.id = 
	commit.interval.ms = 10
	key.serde = class org.apache.kafka.common.serialization.Serdes$StringSerde
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	num.standby.replicas = 0
	num.stream.threads = 1
	partition.grouper = class org.apache.kafka.streams.processor.DefaultPartitionGrouper
	poll.ms = 100
	replication.factor = 1
	state.cleanup.delay.ms = 60000
	state.dir = /tmp/kafka-streams
	timestamp.extractor = class org.apache.kafka.streams.processor.ConsumerRecordTimestampExtractor
	value.serde = class org.apache.kafka.common.serialization.Serdes$ByteArraySerde
	zookeeper.connect = 

[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating producer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.producer.ProducerConfig  - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	block.on.buffer.full = false
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	buffer.memory = 33554432
	client.id = dobby-1-StreamThread-1-producer
	compression.type = none
	connections.max.idle.ms = 540000
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 100
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.fetch.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	timeout.ms = 30000
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = dobby
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.streams.processor.internals.StreamPartitionAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] WARN  org.apache.kafka.clients.consumer.ConsumerConfig  - The configuration 'replication.factor' was supplied but isn't a known config.
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Creating restore consumer client for stream thread [StreamThread-1]
[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.clients.consumer.ConsumerConfig  - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [dev-anta-tools04.kendall.corp.akamai.com:9092]
	check.crcs = true
	client.id = dobby-1-StreamThread-1-restore-consumer
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = 
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka version : 0.10.2.0
[main] INFO  org.apache.kafka.common.utils.AppInfoParser  - Kafka commitId : 576d93a8dc0cf421
[main] INFO  org.apache.kafka.streams.KafkaStreams  - Started Kafka Stream process
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Starting stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Discovered coordinator dev-anta-tools04.kendall.corp.akamai.com:9092 (id: 2147483643 rack: null) for group dobby.
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Revoking previously assigned partitions [] for group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - (Re-)joining group dobby
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.AbstractCoordinator  - Successfully joined group dobby with generation 13
[StreamThread-1] INFO  org.apache.kafka.clients.consumer.internals.ConsumerCoordinator  - Setting newly assigned partitions [gin-0] for group dobby
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamTask  - Creating restoration consumer client for stream task #0_0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 22
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"307","record_tag":"r","timestamp":1.497547981E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 2
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"2ba","record_tag":"r","timestamp":1.497548052E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"358","record_tag":"r","timestamp":1.4975481E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 4
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"255","record_tag":"r","timestamp":1.497548506E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 5
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"365","record_tag":"r","timestamp":1.497548532E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"20a","record_tag":"r","timestamp":1.497548703E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 7
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 7
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"355","record_tag":"r","timestamp":1.49754876E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 0
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 8
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 8
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"2ba","record_tag":"r","timestamp":1.497548862E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 9
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 9
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"236","record_tag":"r","timestamp":1.497548976E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 10
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 10
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"8","record_tag":"r","timestamp":1.497549162E12}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Pushin... :
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - ExecutionTime : 3
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Message in the current stream : 1
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total messages : 11
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - Total stream process: 11
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - output:s6
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - map: {1234=ama-121, 365048=C-GCTYBT}
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - 365048
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - true
[StreamThread-1] INFO  com.aps.akamill.StreamProcessor  - val:{"country_code":"-","cpcode":365048,"city":"-","region_id":0,"http_status":"","object_status":"meV","error":"-","edge_ip":"172.24.51.99","request_id":"317","record_tag":"r","timestamp":1.497549197E12}
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Shutting down stream thread [StreamThread-1]
[StreamThread-1] INFO  org.apache.kafka.clients.producer.KafkaProducer  - Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Removing a task 0_0
[StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - Stream thread shutdown complete [StreamThread-1]
[Thread-1] INFO  org.apache.kafka.streams.KafkaStreams  - Stopped Kafka Stream process
